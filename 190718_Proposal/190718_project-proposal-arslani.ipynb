{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone proposal - Deep Learning for LEPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem to be solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What are the main project idea and goals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By thinking about the capstone project, one emerged when discussing with prof. Yves Hausser from the nature management department at hepia (HES-SO Geneva) about a deep learning project of natural images. As follows, the introduction of this project is presented:\n",
    "\n",
    "The Lepus software [1] was designed to help scientists analyse wildlife images acquired using photographic traps. At present, species recognition and individual identification are carried out manually, which is time-consuming.\n",
    "\n",
    "The objective of this project is to test Deep Learning technology to automate certain tasks such as\n",
    "\n",
    "1. detecting the presence or not of an animal in the image\n",
    "2. locating animals using bounding boxes \n",
    "3. identifying certain species or more generally its type or family\n",
    "4. ideally, identify each individual animal of a specific specie with respect to physical characteristics (e.g. to help the Wildlife Conservation Society)\n",
    "\n",
    "Each of these problematic can be independent and with another Extension EPFL school learner (*Julien Smets*), we decided to share this project. Here are the chosen configurations for our specific capstone projects:\n",
    "\n",
    "**Project 1** *Blerim Arslani : Detection of the presence of an animal in the image (binomial classification)*\n",
    "\n",
    "**Project 2** *Julien Smets : Identification of the type of an animal in the image (multinomial classification)*\n",
    "\n",
    "Please let us shortly motivate our choice. By solving these two problematic, the saved time for nature management scientists could be very high (days of work), especially for the presence detection problem because only a small number of images contain animals (this will be more deeply detailed below).\n",
    "Moreover, the labelled data do not include the bounding boxes which excludes the second project (here we consider only supervised learning to ensure a validation metric) and the fourth one is limited to the significant inspection variance of the manual identification and the very low amount of data available. \n",
    "\n",
    "Note that these two chosen distinct projects are individual (will not be team-based but only the dataset will) and can be combined together by performing the second work right after the first one in order to classify the detected animals (cf. diagram as follows).\n",
    "\n",
    "![General view and separation between the two projects](drawioDiagram.png)\n",
    "\n",
    "\n",
    "As follows, you can see the project cloud for more details: \n",
    "\n",
    "- [1] http://ec2-35-157-78-9.eu-central-1.compute.amazonaws.com/\n",
    "\n",
    "The first project focuses only on the detection of an animal in the pictures. To perform this application and by looking at the data one question arises. Is there a difference in the final result if RGB or grayscale images are used ? And even more important, what is the gain or the loss obtained after using the difference between 2 images following in a sequence ? Grayscale images differences or RBG images differences? These are the main project ideas and goals. Effectively, if 2 images follow one after the other with a relatively short amount of time separating them, the background would substract and the result image would focus on the differences between them. For example, the movement of an animal.\n",
    "\n",
    "To perform these analyses, our machine learning models will be trained on :\n",
    "- Grayscale images\n",
    "- Colour images\n",
    "- Grayscale images differences\n",
    "- Colour images differences\n",
    "\n",
    "A second question would be which model to use ? Is there a model which is more efficient with images differences than row images ? What about grayscale and RBG ? For this purpose several models will be used.\n",
    "- k-nearest neighbours\n",
    "- Decision trees and random forest\n",
    "- Support vector machines\n",
    "- Fully connected neural networks\n",
    "- Convolutional neural network\n",
    "\n",
    "As we saw during the fourth project, training over well define features increase the overall accuracy of the machine learning models. For this purpose and to keep the scope of the project on the main goals, the pixels will be used as input for each model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What story you would like to tell with the data and what would you like to achieve at the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first project consists of detecting the presence or absence of an animal in the pictures. To achieve this goal, several techniques can be used. One of them is to work with the pixel difference intensity between 2 images of a sequence. In a typical case, an animal can be present in the first image and not in the second. A second case, represented in the following images, can be the presence of an animal in the first picture and the same animal with a different position in the following one.\n",
    "\n",
    "![Example of image sequence](images/image_proposal_substraction.png)\n",
    "\n",
    "As the background doesn't change between the first and the second picture, it accentuates the differences between them and so, the animal features. Using the image difference, 4 cases emerge :\n",
    "\n",
    "No animals in both pictures:\n",
    "\n",
    "![first case: no animals in both pictures](images/image_proposal_case_1.png)\n",
    "\n",
    "No animal on the first picture and at least one on the second:\n",
    "\n",
    "![Second case: no animal on the first picture and one on the second](images/image_proposal_case_2.png)\n",
    "\n",
    "At least one animal on the first picture and none on the second:\n",
    "\n",
    "![Third case: one animal on the first picture and none on the second](images/image_proposal_case_3.png)\n",
    "\n",
    "At least one animal on the first and the second picture:\n",
    "\n",
    "![Fourth case: one animal on the first and the second picture](images/image_proposal_case_4.png)\n",
    "\n",
    "Compared to the row images which have two possible labels, the image differences can have four different labels. By choosing appropriate sequences of images, it could be possible to find out the presence of an animal for an individual image looking at her corresponding image difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) What is the main motivation behind your project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea following Lepus project is to reduce the high time consuming manual detection, localisation and identification (and ideally with better accuracy than humans but it will not be experimented here) of animal species in photographic traps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is the size and format of the data that you plan to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data Information*\n",
    "\n",
    "The given species can be very small depending on the animal size and its distance from the camera or very large taking a large part of the image. Animals can be **occluded** by background objects (trees) to even be **partially viewed** (especially for large animals such as giraffes or elephants). The number of species is in proportion irregular depending on the rarity of these species. Moreover, a majority of the given images doesn't contain any animals due to trees movement, dust tornadoes, butterflies, etc. causing false positive captures. The proportion of empty pictures is **~60-85%** depending on the device environment.\n",
    "\n",
    "In addition, cameras have many different fields of view (FOV) and resolutions including colour and gray level images. The latter comes from the difference between day and night acquisition devices. Note that these results in **highly non-uniform representation of species** w.r.t. different situations (day/night, background situation, etc.). As an example some species are only nocturne. This will need robust data preprocessing in order to avoid biases.\n",
    "\n",
    "Some of the images are time correlated due to animals running and get captured several times, i.e. **small-time lapse images**. These set of images are already grouped by the Lepus software in specific folders ready to process. These grouped images are **called events independent capture (EIC)** and numbered from 3 up to ~300 if an animal stays in front of the camera the whole day/night. This can be an important information and will be discussed after.\n",
    "\n",
    "#### *Images*\n",
    "\n",
    "The first data set received \"M1 2015\" (M1 means a grid corresponding to a set of 36 photographic traps) contains 4056 colour and grayscale images. Only ~12% of the data obtained contain animals or humans on them. The rest of the data will be received soon, representing a covered area of 10'000 km$^2$ with hundreds of cameras placed in different nature spots in Tanzania. It represents a period starting from 2013 to 2016 with 176'000 images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4057, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>session_dir</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>file_period</th>\n",
       "      <th>event_id</th>\n",
       "      <th>prev_file_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>place_id</th>\n",
       "      <th>taxon_id</th>\n",
       "      <th>taxon_tsn</th>\n",
       "      <th>taxon_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015/M1/M1_01/CDY_0001.JPG</td>\n",
       "      <td>M1 2015</td>\n",
       "      <td>09.03.15 17:08</td>\n",
       "      <td>day</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015/M1/M1_01/CDY_0002.JPG</td>\n",
       "      <td>M1 2015</td>\n",
       "      <td>10.03.15 12:13</td>\n",
       "      <td>day</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2015/M1/M1_01/CDY_0003.JPG</td>\n",
       "      <td>M1 2015</td>\n",
       "      <td>28.03.15 17:38</td>\n",
       "      <td>day</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Team</td>\n",
       "      <td>Team</td>\n",
       "      <td>[TEAM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2015/M1/M1_02/03080001.JPG</td>\n",
       "      <td>M1 2015</td>\n",
       "      <td>08.03.15 16:57</td>\n",
       "      <td>day</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Team</td>\n",
       "      <td>Team</td>\n",
       "      <td>[TEAM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2015/M1/M1_02/03080002.JPG</td>\n",
       "      <td>M1 2015</td>\n",
       "      <td>08.03.15 16:57</td>\n",
       "      <td>day</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Team</td>\n",
       "      <td>Team</td>\n",
       "      <td>[TEAM]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_id                   file_path session_dir   file_datetime  \\\n",
       "0        1  2015/M1/M1_01/CDY_0001.JPG     M1 2015  09.03.15 17:08   \n",
       "1        2  2015/M1/M1_01/CDY_0002.JPG     M1 2015  10.03.15 12:13   \n",
       "2        3  2015/M1/M1_01/CDY_0003.JPG     M1 2015  28.03.15 17:38   \n",
       "3        4  2015/M1/M1_02/03080001.JPG     M1 2015  08.03.15 16:57   \n",
       "4        5  2015/M1/M1_02/03080002.JPG     M1 2015  08.03.15 16:57   \n",
       "\n",
       "  file_period  event_id  prev_file_id  session_id  place_id taxon_id  \\\n",
       "0         day         1           NaN           5         1      NaN   \n",
       "1         day         2           NaN           5         1      NaN   \n",
       "2         day         3           NaN           5         1     Team   \n",
       "3         day         4           NaN           5         2     Team   \n",
       "4         day         4           4.0           5         2     Team   \n",
       "\n",
       "  taxon_tsn taxon_name  \n",
       "0       NaN        NaN  \n",
       "1       NaN        NaN  \n",
       "2      Team     [TEAM]  \n",
       "3      Team     [TEAM]  \n",
       "4      Team     [TEAM]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import images\n",
    "# Location of all documents\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "absFilePath = os.path.abspath(os.getcwd())\n",
    "#print(absFilePath)\n",
    "#fileDir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data = pd.read_csv(os.path.join(absFilePath, 'DeepLearningExport.csv'), sep = ',', header = 'infer')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images information is given by a CSV file. Following the column information. Taxon meaning species.\n",
    "\n",
    "- **file id**: index of the images\n",
    "- **file_path**: path to reach the image: {year}/{grid}/{camera}/{picture}\n",
    "- **session_dir**: grid + years\n",
    "- **file_datetime**: date and time\n",
    "- **file_period**: day, night, twilight\n",
    "- **event_id**: identifier of each event\n",
    "- **prev_file_id**: identifier of the chronologically previous image for the same event_id.\n",
    "- **session_id**: session identifier\n",
    "- **place_id**: place identifier\n",
    "- **taxon_id**: species identifier\n",
    "- **taxon_tsn**: unique identifier of an official database of the living world\n",
    "- **taxon_name**: literal name of the species. May have the value \"[TEAM]\" if it is the human team captured.\n",
    "\n",
    "Each event identifier contains from one to hundreds of pictures. Based on the file data time, each file_id is linked to the previous file identifier. If the value is a NaN, the picture is the first of the event. To work with images differences, we need at least 2 images per independent event.\n",
    "\n",
    "![Histogram of the number of images by independent events](images/histogram_events.png)\n",
    "\n",
    "Looking at the histogram of the number of images per independent events, it is clear that a large majority of independent events contain fewer than 10 images. Zooming on the histogram, almost half of the events contain between 2 and 3 images. Fifty of them contain 1 image and are not usable for image differences.\n",
    "\n",
    "It is also important to look at the species present in the data set.\n",
    "\n",
    "\n",
    "![Cumulative plot of the percentage of the data containing species](images/cumulative_plot_species.png)\n",
    "\n",
    "Thirty-one different species represent in the data represent 12.5% of the data. The limit of red dashed line on the graph. The first 3 represent in quantity almost half of the species present in the data.\n",
    "\n",
    "![3 Top species present in the data](images/representation_top_3_species.png)\n",
    "\n",
    "It is interesting to observe that the most present species is seen during the day. In opposition to the third one which is mostly present hunting at night. Also very interestingly, the second most present species is the human in charge of the traps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Other*\n",
    "\n",
    "Note that the data can need to be confidential with DNA (standard Non-Disclosure Agreement) due to some very rare species which are often hunted for money. If this is the case, it will be demanded to the EPFL soon. But anyways the precise location of the picture will not be transmitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) How do you expect to get, manage and process the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Receiving the data*\n",
    "the first grid of data is shared within a switch drive transfer. The rest of the data is shared by physical transfer in order to conserve privacy. \n",
    "\n",
    "#### *Managing the data*\n",
    "In resume the size of the data represent approximately.\n",
    "- 99.5 Go\n",
    "- 179'000 images\n",
    "\n",
    "As the data will be trained on colour, grayscale, colour difference and grayscale difference. The final data set will go between 300 and 400 Go. This quantity is too big for my own capacity of storage. The solution is to resize the resolution of the images.\n",
    "\n",
    "#### *Data pre-processing*\n",
    "**Colour images**: The column 'taxon_name' from the csv file gives the name of the species present in the image and 'NaN' for an empty image. The first step is to create a column 'presence' with two labels, empty or animal.\n",
    "\n",
    "**Grayscale images**: The colour images have to be first transform into grayscale based on the following equation[1]: $L = R * 299/1000 + G * 587/1000 + B * 114/1000$\n",
    "\n",
    "'R' represents the red component, 'G' the green and 'B' blue.\n",
    "Similarly than for the colour images, a column 'presence' is added with two labels, empty or animals.\n",
    "\n",
    "- [1] https://pillow.readthedocs.io/en/3.2.x/reference/Image.html#PIL.Image.Image.convert\n",
    "\n",
    "**Colour differences**: Based on the colour images and the columns 'file_id' and 'prev_file_id' each image can be referred to her own previous images. This is possible only if the images are part of the same type and the same event. How to know which images are part of an event ? The column 'event_id' index each independent event between 1 and 552. There are fewer events than pictures because some events can go up to several hundreds of pictures. The first image of an event is referred as 'NaN' in the 'prev_file_id' and is not used as a reference for the image differences. As two images are needed only events with 2 or more pictures are taken into account. As mentioned before, image differences take two images on the input and reduce it into one by subtracting the pixel intensity of a first picture considered as a reference to the previous one. This process aggregate the information of two pictures into one and focuses only on the differences between them. As the results is needed for each picture, a column 'presence' is added. The labels are quite different than the one for colour and grayscale images. \n",
    "\n",
    "For each picture pair, the label will be\n",
    "\n",
    "- *NoAndNo*: no animals on both pictures\n",
    "- *NoAndYes*: no animal on the first picture and at least one on the second\n",
    "- *YesAndNo*: at least one animal on the first picture and none on the second\n",
    "- *YesAndYes*: at least one animal on the first and the second picture\n",
    "\n",
    "**Grayscale differences**: Grayscale differences are very similar to colour differences. The only difference is that are based on the grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What are the main challenges that you envision for completing the project and how do you plan to get around each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment modification**: Due to weather conditions and unvolontair factors, the background can change for the same trap leading to a significant modification of the environment. To counter this issue only images from the same independent event will be used. In addition, only images following in a period of time smaller than the time for change to occur will be used.\n",
    "\n",
    "**Independent events with only one image**: As we saw previously, some events only contain one image. These events can't be used in case of the difference method and won't be used in the data set.\n",
    "\n",
    "**Resolution of the images**: The resolution of the images is too large for the application (1536, 2048, 3) resulting in 3'145'728 features. To handle this issue, the resolution of each image will be reduced to (192, 256, 3), leading to 49'152 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What are the steps that you plan to take to achieve the end goals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of the work are given as follows:\n",
    "\n",
    "1. Preprocess the dataset\n",
    "\n",
    "2. Split the data in train and test sets.\n",
    "\n",
    "3. Test different models of machine learning by tuning their hyperparameters.\n",
    "\n",
    "4. Compare the results.\n",
    "\n",
    "5. Define future improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Show us that you have a pipeline in place and that you understand the feasibility of your project goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1. Preprocessing: \\**\n",
    "- Resizing the original data\n",
    "- Creating grayscale data set\n",
    "- Creating colour difference data set\n",
    "- Creating grayscale difference data set\n",
    "- Removing all images which are used with is not labelled by difference method\n",
    "- Applying some preprocessing methods on the data sets (e.g.* histogram equalisation, denying, resizing, data augmentation such as noise, horizontal flip, rotations or shear).\n",
    "\n",
    "#### *2. Splitting the data:*\n",
    "- Split the data into stratified train, test sets.\n",
    "\n",
    "#### *3. Fit and evaluate models*\n",
    "- k-nearest neighbours\n",
    "- Decision trees and random forest\n",
    "- Support vector machines\n",
    "- Fully connected neural networks\n",
    "- Convolutional neural network\n",
    "\n",
    "#### *4. Compare the results*\n",
    "- Evaluate the accuracy of each model with each data set.\n",
    "\n",
    "#### *5. Define future improvements based on the results*\n",
    "- Based on the results observe the kinds of images with which models have a problem and find out a preprocessing improvement.\n",
    "- Define a baseline model from literature which could develop features to inject at the inlet of our models instead of the images pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sample will be implemented in several python scripts (.py) for convenience and simplicity (e.g. separation of pre-processing, training and testing). An additional notebook (.ipynb) will be included with analysis details and visualisation figures as a small report (similarly to this document)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
